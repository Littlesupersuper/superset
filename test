在 Java 的 Spark 程序中，可以通过 JDBC 将 Dataset 写入到 ClickHouse。以下是实现的步骤和示例代码：

1. 添加依赖

在你的 Maven 项目中添加 ClickHouse JDBC 驱动和 Spark 的相关依赖。

Maven 依赖

<dependencies>
    <!-- Spark Core -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_2.12</artifactId>
        <version>3.4.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_2.12</artifactId>
        <version>3.4.0</version>
    </dependency>
    
    <!-- ClickHouse JDBC -->
    <dependency>
        <groupId>com.clickhouse</groupId>
        <artifactId>clickhouse-jdbc</artifactId>
        <version>0.4.6</version>
    </dependency>
</dependencies>

2. 编写代码

示例代码

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import java.util.Properties;

public class SparkToClickHouse {
    public static void main(String[] args) {
        // 1. 初始化 SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("Write to ClickHouse")
                .master("local[*]") // 本地运行模式
                .getOrCreate();

        // 2. 创建示例数据
        Dataset<Row> dataset = spark.read().json("path/to/input.json"); // 替换为你的数据源

        // 打印数据（可选）
        dataset.show();

        // 3. 定义 ClickHouse 的连接配置
        String clickhouseUrl = "jdbc:clickhouse://<host>:<port>/<database>";
        Properties connectionProperties = new Properties();
        connectionProperties.put("user", "default");
        connectionProperties.put("password", ""); // 默认密码为空
        connectionProperties.put("driver", "com.clickhouse.jdbc.ClickHouseDriver");

        // 4. 写入到 ClickHouse
        String targetTable = "<your_table_name>"; // 目标表名
        dataset.write()
                .mode("append") // 写入模式：append, overwrite, etc.
                .jdbc(clickhouseUrl, targetTable, connectionProperties);

        // 5. 停止 SparkSession
        spark.stop();
    }
}

3. 配置说明

3.1 数据源

	•	将 path/to/input.json 替换为实际的数据源路径，例如 JSON 文件、CSV 文件，或者已经加载的 Dataset。

3.2 ClickHouse 连接

	•	jdbc:clickhouse://<host>:<port>/<database>：
	•	<host>：ClickHouse 的主机名或 IP。
	•	<port>：ClickHouse 的端口，默认是 8123。
	•	<database>：ClickHouse 中的目标数据库。
	•	user 和 password：替换为 ClickHouse 的用户名和密码。

3.3 写入模式

	•	.mode("append")：追加数据到表中。
	•	.mode("overwrite")：覆盖目标表中的数据。
	•	.mode("ignore")：如果目标表已存在，忽略当前操作。

3.4 ClickHouse 表结构

确保目标表的结构与 Dataset 的 schema 匹配，例如字段名和数据类型一致。

4. 注意事项

4.1 性能优化

ClickHouse 是为批量写入优化的数据库，建议：
	•	将数据分区写入以提高性能。
	•	调整 Spark 的分区数：

dataset.repartition(10); // 例如分成 10 个分区



4.2 类型匹配

Spark 和 ClickHouse 的数据类型需要匹配，常见类型映射：

Spark 数据类型	ClickHouse 数据类型
String	String
Integer	Int32
Long	Int64
Double	Float64
Timestamp	DateTime

如果有不匹配，可以使用 Spark 的 withColumn 方法转换列类型。

4.3 错误处理

	•	如果出现 Connection refused：
	•	检查 ClickHouse 是否启动，端口是否正确。
	•	如果表不存在：
	•	在 ClickHouse 中手动创建表，或者通过 Spark SQL 创建表。

5. 测试示例

假设我们有以下 JSON 数据文件 data.json：

[
  {"id": 1, "name": "Alice", "age": 30},
  {"id": 2, "name": "Bob", "age": 25}
]

在 ClickHouse 中创建表：

CREATE TABLE my_table (
    id Int32,
    name String,
    age Int32
) ENGINE = MergeTree()
ORDER BY id;

运行上述程序后，数据将被写入 ClickHouse 的 my_table 中。

6. 总结

	•	使用 dataset.write().jdbc() 可以轻松将 Spark 数据写入 ClickHouse。
	•	优化写入性能时，需调整 Spark 的分区数和写入模式。
	•	确保数据的 schema 和 ClickHouse 表结构匹配，避免类型不一致导致的错误。

如果遇到问题，可以提供日志信息，进一步排查。