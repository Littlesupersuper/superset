从错误信息来看，问题的根本原因是 Spark JDBC 无法将 Map<String, Float> 类型映射到 JDBC 支持的类型。Spark 默认支持的 JDBC 类型中没有 Map 类型的直接对应类型，因此在保存到数据库时会抛出 SparkIllegalArgumentException。

以下是解决此问题的几种方法：

1. 转换 Map 列为 JSON 格式

Map 类型无法直接存储到数据库，但可以将其转换为 JSON 字符串，然后存储为 String 类型。示例如下：

代码实现

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;

public class SparkToClickHouse {
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("Write to ClickHouse with Map")
                .master("local[*]")
                .getOrCreate();

        // 假设数据已经加载到 Dataset 中
        Dataset<Row> dataset = spark.read().json("path/to/input.json");

        // 将 Map 列转换为 JSON 字符串列
        Dataset<Row> transformedDataset = dataset.withColumn(
                "map_as_json",
                functions.to_json(dataset.col("map_column")) // 替换 map_column 为实际列名
        );

        // 移除原始的 Map 列，或者重命名 JSON 列
        transformedDataset = transformedDataset.drop("map_column");

        // 写入到 ClickHouse
        String clickhouseUrl = "jdbc:clickhouse://<host>:<port>/<database>";
        String targetTable = "<your_table_name>";

        transformedDataset.write()
                .mode("append")
                .jdbc(clickhouseUrl, targetTable, getJdbcProperties());
    }

    private static java.util.Properties getJdbcProperties() {
        java.util.Properties properties = new java.util.Properties();
        properties.put("user", "default");
        properties.put("password", "");
        properties.put("driver", "com.clickhouse.jdbc.ClickHouseDriver");
        return properties;
    }
}

2. 修改 ClickHouse 表的字段类型

在 ClickHouse 表中将目标字段定义为 String 类型（例如存储 JSON）或其他可以接受的格式。示例：

ClickHouse 表结构

CREATE TABLE my_table (
    id Int32,
    map_column String -- 用于存储 JSON 格式的 Map 数据
) ENGINE = MergeTree()
ORDER BY id;

解释

	•	将 Map<String, Float> 转换为 JSON 字符串后，存储为 String 类型。
	•	在查询时，ClickHouse 提供函数如 JSONExtract 来解析 JSON 数据：

SELECT JSONExtract(map_column, 'key', 'Float32') FROM my_table;

3. 转换为结构化列

如果 Map 数据是固定的（即键的集合已知且数量有限），可以将其拆分成多个单独的列。

代码实现

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import static org.apache.spark.sql.functions.col;

public class SparkToClickHouse {
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("Write to ClickHouse with Map to Columns")
                .master("local[*]")
                .getOrCreate();

        // 假设数据已经加载到 Dataset 中
        Dataset<Row> dataset = spark.read().json("path/to/input.json");

        // 将 Map 转换为单独的列
        Dataset<Row> transformedDataset = dataset
                .withColumn("key1", col("map_column.key1"))
                .withColumn("key2", col("map_column.key2"))
                .drop("map_column"); // 删除原始 Map 列

        // 写入到 ClickHouse
        String clickhouseUrl = "jdbc:clickhouse://<host>:<port>/<database>";
        String targetTable = "<your_table_name>";

        transformedDataset.write()
                .mode("append")
                .jdbc(clickhouseUrl, targetTable, getJdbcProperties());
    }

    private static java.util.Properties getJdbcProperties() {
        java.util.Properties properties = new java.util.Properties();
        properties.put("user", "default");
        properties.put("password", "");
        properties.put("driver", "com.clickhouse.jdbc.ClickHouseDriver");
        return properties;
    }
}

4. 配置自定义数据类型支持

如果使用 Map 是必需的（例如动态键值对），可以通过实现 Spark 自定义数据类型映射来支持。但这需要深入了解 Spark 和 JDBC 的类型转换机制，一般来说复杂度较高且不推荐。

总结

	•	推荐方法 1： 将 Map 转换为 JSON 字符串（通用且简单）。
	•	推荐方法 2： 如果 Map 的键是固定的，可以拆分为单独的列。
	•	注意： 数据库的表结构需与转换后的数据结构匹配。

如问题依然存在，可提供具体的 Map 数据和目标表结构，我可以进一步协助解决。